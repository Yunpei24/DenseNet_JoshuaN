{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6799,"databundleVersionId":4225553,"sourceType":"competition"},{"sourceId":12881211,"sourceType":"datasetVersion","datasetId":8149460}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport shutil\nfrom tqdm import tqdm\n\nimport wandb\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:25.764666Z","iopub.execute_input":"2025-08-27T02:06:25.764953Z","iopub.status.idle":"2025-08-27T02:06:28.196366Z","shell.execute_reply.started":"2025-08-27T02:06:25.764932Z","shell.execute_reply":"2025-08-27T02:06:28.195760Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Log in to W&B using the API key stored in Kaggle Secrets\ntry:\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n    wandb.login(key=wandb_api_key)\n    WANDB_ACTIVE = True\n    print(\"Successfully logged into W&B.\")\nexcept:\n    WANDB_ACTIVE = False\n    print(\"WARNING: Could not log into W&B. Training will continue without tracking.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:31.185132Z","iopub.execute_input":"2025-08-27T02:06:31.185711Z","iopub.status.idle":"2025-08-27T02:06:37.888950Z","shell.execute_reply.started":"2025-08-27T02:06:31.185685Z","shell.execute_reply":"2025-08-27T02:06:37.888234Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjnikiema\u001b[0m (\u001b[33mimg_seg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"Successfully logged into W&B.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_validation_set_from_kaggle(input_dir, output_dir):\n    \"\"\"\n    Prepares the ImageNet validation set from the pre-extracted Kaggle directory structure.\n    \n    Args:\n        input_dir (str): The root path to the Kaggle input data.\n        output_dir (str): The path to the writeable output directory (/kaggle/working/).\n    \"\"\"\n    \n    print(\"--- Preparing ONLY the validation set ---\")\n    \n    # Define paths\n    val_images_path = os.path.join(input_dir, 'ILSVRC/Data/CLS-LOC/val')\n    solution_file_path = os.path.join(input_dir, 'LOC_val_solution.csv')\n    \n    # The new sorted validation directory will be created in our workspace\n    sorted_val_dir = os.path.join(output_dir, 'val_sorted')\n    os.makedirs(sorted_val_dir, exist_ok=True)\n    \n    print(f\"Reading solution file from: {solution_file_path}\")\n    # Read the CSV into a pandas DataFrame\n    df = pd.read_csv(solution_file_path)\n    \n    # The PredictionString contains the class ID (e.g., 'n01440764 1 2 3 4')\n    # We just need the first part.\n    df['class_id'] = df['PredictionString'].apply(lambda x: x.split(' ')[0])\n    \n    print(f\"Found {len(df)} images to sort.\")\n    print(f\"Copying and sorting images from {val_images_path} to {sorted_val_dir}...\")\n\n    # Loop through the dataframe and copy each file to its new class directory\n    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n        image_id = row['ImageId']\n        class_id = row['class_id']\n        \n        # Create the destination class folder if it doesn't exist\n        dest_class_dir = os.path.join(sorted_val_dir, class_id)\n        os.makedirs(dest_class_dir, exist_ok=True)\n        \n        # Construct source and destination paths\n        src_path = os.path.join(val_images_path, image_id + '.JPEG')\n        dest_path = os.path.join(dest_class_dir, image_id + '.JPEG')\n        \n        # Copy the file\n        shutil.copyfile(src_path, dest_path)\n        \n    print(\"\\n--- Validation set preparation complete! ---\")\n    print(f\"Sorted validation data is ready in: {sorted_val_dir}\")\n    return sorted_val_dir","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:46.933477Z","iopub.execute_input":"2025-08-27T02:06:46.933904Z","iopub.status.idle":"2025-08-27T02:06:46.940777Z","shell.execute_reply.started":"2025-08-27T02:06:46.933882Z","shell.execute_reply":"2025-08-27T02:06:46.940033Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim","metadata":{"id":"EsDQCybacFyR","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:47.154965Z","iopub.execute_input":"2025-08-27T02:06:47.155183Z","iopub.status.idle":"2025-08-27T02:06:47.159185Z","shell.execute_reply.started":"2025-08-27T02:06:47.155166Z","shell.execute_reply":"2025-08-27T02:06:47.158490Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Training on device: {device}\")","metadata":{"id":"Naw3-f1Rtnko","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:47.391920Z","iopub.execute_input":"2025-08-27T02:06:47.392142Z","iopub.status.idle":"2025-08-27T02:06:47.396069Z","shell.execute_reply.started":"2025-08-27T02:06:47.392125Z","shell.execute_reply":"2025-08-27T02:06:47.395363Z"}},"outputs":[{"name":"stdout","text":"Training on device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# I. Building Blocks of DenseNet","metadata":{"id":"2MQwHm6nktNt"}},{"cell_type":"markdown","source":"We'll start by implementing the core components of the DenseNet architecture.","metadata":{"id":"6DBgMjuPkwwK"}},{"cell_type":"markdown","source":"## 1. DenseNet Simple Layer","metadata":{"id":"w4HHFGQHZed2"}},{"cell_type":"markdown","source":"The simple layer in a DenseNet consists of a Batch Normalization layer, a ReLU activation function, and a 3x3 Convolutional layer.","metadata":{"id":"rQDqWo7NZY4e"}},{"cell_type":"code","source":"class DenseNetSimpleLayer(nn.Module):\n  def __init__(self, in_channels, growth_rate):\n      \"\"\"\n      Initializes the DenseNet Simple Layer.\n\n      Args:\n          in_channels (int): Number of input channels.\n          growth_rate (int): Number of output channels (k in the paper).\n      \"\"\"\n      super(DenseNetSimpleLayer, self).__init__()\n      self.bn1 = nn.BatchNorm2d(in_channels)\n      self.relu1 = nn.ReLU(inplace=True)\n      self.conv1 = nn.Conv2d(in_channels, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n\n  def forward(self, x):\n      \"\"\"\n      Forward pass of the DenseNet Simple Layer.\n\n      Args:\n          x (torch.Tensor): Input tensor.\n\n      Returns:\n          torch.Tensor: Output tensor.\n      \"\"\"\n      out = self.conv1(self.relu1(self.bn1(x)))\n      out = torch.cat([x, out], 1)\n      return out","metadata":{"id":"ufK29vohYtax","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:48.592195Z","iopub.execute_input":"2025-08-27T02:06:48.592463Z","iopub.status.idle":"2025-08-27T02:06:48.597529Z","shell.execute_reply.started":"2025-08-27T02:06:48.592442Z","shell.execute_reply":"2025-08-27T02:06:48.596886Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 2. DenseNet Bottleneck Layer","metadata":{"id":"uKiqfG3YZiw_"}},{"cell_type":"markdown","source":"The bottleneck layer is a more computationally efficient version of the simple layer. It introduces a 1x1 convolution to reduce the number of feature maps before the more expensive 3x3 convolution. The 1x1 convolution produces 4 * growth_rate feature maps.","metadata":{"id":"BPbZ_XHRZonA"}},{"cell_type":"code","source":"class DenseNetBottleneckLayer(nn.Module):\n  def __init__(self, in_channels, growth_rate, dropout_rate=0):\n      \"\"\"\n      Initializes the DenseNet Bottleneck Layer.\n\n      Args:\n          in_channels (int): Number of input channels.\n          growth_rate (int): Number of output channels for the 3x3 convolution.\n      \"\"\"\n      super(DenseNetBottleneckLayer, self).__init__()\n      inter_channels = 4 * growth_rate\n      self.dropout_rate = dropout_rate\n      self.bn1 = nn.BatchNorm2d(in_channels)\n      self.relu1 = nn.ReLU(inplace=True)\n      self.conv1 = nn.Conv2d(in_channels, inter_channels, kernel_size=1, stride=1, bias=False)\n\n      self.bn2 = nn.BatchNorm2d(inter_channels)\n      self.relu2 = nn.ReLU(inplace=True)\n      self.conv2 = nn.Conv2d(inter_channels, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n      # Add dropout layer if specified\n      if self.dropout_rate > 0:\n          self.dropout = nn.Dropout2d(p=self.dropout_rate)\n          \n  def forward(self, x):\n      out = self.conv1(self.relu1(self.bn1(x)))\n      out = self.conv2(self.relu2(self.bn2(out)))\n      \n      # Apply dropout before concatenation\n      if self.dropout_rate > 0:\n        out = self.dropout(out)\n          \n      out = torch.cat([x, out], 1)\n      return out\n","metadata":{"id":"XV0GBHKpZnys","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:51.965941Z","iopub.execute_input":"2025-08-27T02:06:51.966418Z","iopub.status.idle":"2025-08-27T02:06:51.972660Z","shell.execute_reply.started":"2025-08-27T02:06:51.966393Z","shell.execute_reply":"2025-08-27T02:06:51.971880Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## 3. Transition Layer","metadata":{"id":"hFYGqXe1aiHi"}},{"cell_type":"markdown","source":"The transition layer connects two dense blocks. It consists of a Batch Normalization layer, a 1x1 Convolutional layer to reduce the number of channels (compression), and an Average Pooling layer to reduce the spatial dimensions.","metadata":{"id":"nq3b_s49alH0"}},{"cell_type":"code","source":"class TransitionLayer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        \"\"\"\n        Initializes the Transition Layer.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n        \"\"\"\n        super(TransitionLayer, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n        self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the Transition Layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        out = self.conv1(self.relu1(self.bn1(x)))\n        out = self.avg_pool(out)\n        return out","metadata":{"id":"3DTkPc_rZ_V9","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:52.853201Z","iopub.execute_input":"2025-08-27T02:06:52.853880Z","iopub.status.idle":"2025-08-27T02:06:52.858659Z","shell.execute_reply.started":"2025-08-27T02:06:52.853828Z","shell.execute_reply":"2025-08-27T02:06:52.857949Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# II. Assembling the Full DenseNet Model","metadata":{"id":"BcMn67Guag8g"}},{"cell_type":"markdown","source":"Now we will combine these building blocks to create the complete DenseNet architecture.","metadata":{"id":"9i-iIQkukohX"}},{"cell_type":"code","source":"class DenseNet(nn.Module):\n  def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10, dropout_rate=0, init_weights=True, dataset_used=\"cifar\"):\n    \"\"\"\n    Initializes the DenseNet model.\n\n    Args:\n        block (nn.Module): The type of dense layer to use (Simple or Bottleneck).\n        nblocks (list of int): The number of layers in each dense block.\n        growth_rate (int): The growth rate (k).\n        reduction (float): The compression factor for the transition layers.\n        num_classes (int): The number of output classes.\n        init_weights (bool): Whether to initialize the weights.\n        dataset_used (str): The dataset used for training.\n    \"\"\"\n    super(DenseNet, self).__init__()\n    self.growth_rate = growth_rate\n    num_planes = 2 * growth_rate\n    self.dropout_rate = dropout_rate\n    self.dataset_used = dataset_used\n\n    if self.dataset_used == \"cifar\":\n      # Initial convolution for CIFAR-X\n      self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n    else:\n      # # Initial convolution for ImageNet\n      self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 2 * growth_rate, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(2 * growth_rate),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        )\n\n    # First Dense Block\n    self.dense1 = self._make_dense_block(block, num_planes, nblocks[0])\n    num_planes += nblocks[0] * growth_rate\n    out_planes = int(num_planes * reduction)\n    self.trans1 = TransitionLayer(num_planes, out_planes)\n    num_planes = out_planes\n\n    # Second Dense Block\n    self.dense2 = self._make_dense_block(block, num_planes, nblocks[1])\n    num_planes += nblocks[1] * growth_rate\n    out_planes = int(num_planes * reduction)\n    self.trans2 = TransitionLayer(num_planes, out_planes)\n    num_planes = out_planes\n\n    # Third Dense Block\n    self.dense3 = self._make_dense_block(block, num_planes, nblocks[2])\n    num_planes += nblocks[2] * growth_rate\n      \n    if self.dataset_used != \"cifar\":\n        out_planes = int(num_planes * reduction)\n        self.trans3 = TransitionLayer(num_planes, out_planes)\n        num_planes = out_planes\n    \n        # Fourth Dense Block\n        self.dense4 = self._make_dense_block(block, num_planes, nblocks[3])\n        num_planes += nblocks[3] * growth_rate\n\n    # Final layers\n    self.bn = nn.BatchNorm2d(num_planes)\n    self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n    self.linear = nn.Linear(num_planes, num_classes)\n\n    if init_weights:\n        self._initialize_weights()\n\n  def _make_dense_block(self, block, in_planes, nblock):\n      layers = []\n      for _ in range(nblock):\n          layers.append(block(in_planes, self.growth_rate, self.dropout_rate))\n          in_planes += self.growth_rate\n      return nn.Sequential(*layers)\n\n  def _initialize_weights(self):\n      for m in self.modules():\n          if isinstance(m, nn.Conv2d):\n              nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n              if m.bias is not None:\n                  nn.init.constant_(m.bias, 0)\n          elif isinstance(m, nn.BatchNorm2d):\n              nn.init.constant_(m.weight, 1)\n              nn.init.constant_(m.bias, 0)\n          elif isinstance(m, nn.Linear):\n              nn.init.constant_(m.bias, 0)\n\n  def forward(self, x):\n      out = self.conv1(x)\n      out = self.trans1(self.dense1(out))\n      out = self.trans2(self.dense2(out))\n      if self.dataset_used != \"cifar\":\n          out = self.trans3(self.dense3(out))\n          out = self.dense4(out)\n      else:\n          out = self.dense3(out)\n      out = self.avg_pool(F.relu(self.bn(out)))\n      out = torch.flatten(out, 1)\n      out = self.linear(out)\n      return out","metadata":{"id":"wRil96_4k5xW","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:53.621266Z","iopub.execute_input":"2025-08-27T02:06:53.621892Z","iopub.status.idle":"2025-08-27T02:06:53.634049Z","shell.execute_reply.started":"2025-08-27T02:06:53.621866Z","shell.execute_reply":"2025-08-27T02:06:53.633403Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# III. Training and Evaluation","metadata":{"id":"9NEz5SmMkLWO"}},{"cell_type":"code","source":"def Densenet_cifar(k=12, dropout_rate=0, num_classes=10):\n    return DenseNet(DenseNetBottleneckLayer, [16, 16, 16], growth_rate=k, dropout_rate=dropout_rate, num_classes=num_classes)","metadata":{"id":"lRacr3NYkUuN","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:54.137038Z","iopub.execute_input":"2025-08-27T02:06:54.137463Z","iopub.status.idle":"2025-08-27T02:06:54.141180Z","shell.execute_reply.started":"2025-08-27T02:06:54.137442Z","shell.execute_reply":"2025-08-27T02:06:54.140516Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def DenseNet121():\n    return DenseNet(DenseNetBottleneckLayer, [6,12,24,16], growth_rate=32, dataset_used=\"imagenet\")\n\ndef DenseNet169():\n    return DenseNet(DenseNetBottleneckLayer, [6,12,32,32], growth_rate=32, dataset_used=\"imagenet\")\n\ndef DenseNet201():\n    return DenseNet(DenseNetBottleneckLayer, [6,12,48,32], growth_rate=32, dataset_used=\"imagenet\")\n\ndef DenseNet161():\n    return DenseNet(DenseNetBottleneckLayer, [6,12,36,24], growth_rate=48, dataset_used=\"imagenet\")","metadata":{"id":"ecNTKmqOkaRi","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:54.396876Z","iopub.execute_input":"2025-08-27T02:06:54.397236Z","iopub.status.idle":"2025-08-27T02:06:54.401543Z","shell.execute_reply.started":"2025-08-27T02:06:54.397218Z","shell.execute_reply":"2025-08-27T02:06:54.400971Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## 1. For CIFAR-10 & 100 with simple data augmentation","metadata":{"id":"-Y0quA22k4Qh"}},{"cell_type":"code","source":"transform_train_cifar = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test_cifar = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])","metadata":{"id":"EXoH7CX9tK_X","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:54.993359Z","iopub.execute_input":"2025-08-27T02:06:54.993591Z","iopub.status.idle":"2025-08-27T02:06:54.998302Z","shell.execute_reply.started":"2025-08-27T02:06:54.993572Z","shell.execute_reply":"2025-08-27T02:06:54.997538Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# TRAINING FUNCTION\n\ndef train(epoch, model, trainloader, optimizer, criterion, device):\n        model.train()\n        print(f'\\nEpoch: {epoch} | LR: {optimizer.param_groups[0][\"lr\"]:.5f}')\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        progress_bar = tqdm(enumerate(trainloader), total=len(trainloader))\n        for i, (inputs, targets) in progress_bar:\n            inputs, targets = inputs.to(device), targets.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n            if epoch % 10 == 0:\n                progress_bar.set_description(f'Loss: {running_loss/(i+1):.3f} | Acc: {100.*correct/total:.3f}%')\n\n\n        avg_loss = running_loss / len(trainloader)\n        avg_acc = 100. * correct / total\n        \n        return avg_loss, avg_acc\n\n\n# EVALUATION FUNCTION\n\ndef evaluate_cifar(epoch, model, testloader, criterion, device):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in testloader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n            \n    avg_loss = test_loss / len(testloader)\n    avg_acc = 100. * correct / total\n    error_rate = 100. - avg_acc\n    \n    if epoch % 10 == 0:\n        print(f\"--- Epoch {epoch} Test Results ---\")\n        print(f\"Accuracy: {avg_acc:.2f}% | Error Rate: {error_rate:.2f}% | Loss: {avg_loss:.2f}%\")\n        print(\"--------------------------\")\n\n    return avg_loss, avg_acc","metadata":{"id":"Ks5xscyPtYFf","trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:06:55.265528Z","iopub.execute_input":"2025-08-27T02:06:55.265750Z","iopub.status.idle":"2025-08-27T02:06:55.273850Z","shell.execute_reply.started":"2025-08-27T02:06:55.265731Z","shell.execute_reply":"2025-08-27T02:06:55.273197Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### CIFAR-10","metadata":{}},{"cell_type":"code","source":"EPOCHS = 300\n# DataLoaders\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train_cifar)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test_cifar)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\nfor k in [12, 24]:\n    print(\"#\"*50)\n    print(f\"--- Starting training for k = {k} ---\")\n\n    # Manage the run ID for robust resuming\n    run_id = None\n    run_id_path = f\"/kaggle/working/wandb_run_id_k{k}.txt\"\n    if os.path.exists(run_id_path):\n        with open(run_id_path, 'r') as f:\n            run_id = f.read()\n        print(f\"Found existing run ID for k={k}: {run_id}. Attempting to resume.\")\n\n    # Initialize a new W&B run\n    if WANDB_ACTIVE:\n        run = wandb.init(\n            project=\"densenet-cifar10\",\n            name=f\"densenet_k{k}\",\n            config={\n                \"growth_rate\": k, \"epochs\": EPOCHS, \"batch_size\": 128,\n                \"learning_rate\": 0.1, \"optimizer\": \"SGD_Nesterov\"\n            },\n            id=run_id,\n            resume=\"allow\",\n            job_type=\"train\"\n        )\n    \n    # Save the new run ID if this is a fresh run\n    if WANDB_ACTIVE and not os.path.exists(run_id_path):\n        with open(run_id_path, 'w') as f:\n            f.write(run.id)\n        print(f\"Created new run with ID: {run.id}. Saved ID for future resume.\")\n\n    # Create the model, optimizer, and scheduler\n    model = Densenet_cifar(k=k).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4, nesterov=True)\n    milestones = [int(EPOCHS * 0.5), int(EPOCHS * 0.75)]\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.1)\n\n    start_epoch = 0\n\n    # Attempt to resume from a W&B checkpoint if the run was successfully resumed\n    if WANDB_ACTIVE and run.resumed:\n        try:\n            print(\"Run successfully resumed. Attempting to load checkpoint from W&B Artifacts...\")\n            # Fetch the latest version of the artifact\n            artifact = run.use_artifact(f'densenet-cifar-k{k}:latest')\n            # Download the checkpoint file\n            artifact_dir = artifact.download(root=\"/kaggle/working/artifacts\")\n            checkpoint_file = os.path.join(artifact_dir, os.listdir(artifact_dir)[0])\n\n            # Load the state of the model, optimizer, etc.\n            checkpoint = torch.load(checkpoint_file)\n            model.load_state_dict(checkpoint['model_state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            start_epoch = checkpoint['epoch'] + 1\n            print(f\"Checkpoint loaded. Resuming training at epoch {start_epoch}.\")\n        except Exception as e:\n            print(f\"Could not load checkpoint. Starting from scratch. Error: {e}\")\n\n    # Main Training Loop\n    for epoch in range(start_epoch, EPOCHS):\n        # Training and evaluation\n        train_loss, train_acc = train(epoch, model, trainloader, optimizer, criterion, device)\n        test_loss, test_acc = evaluate_cifar(epoch, model, testloader, criterion, device)\n        scheduler.step()\n\n        # Log metrics to W&B\n        if WANDB_ACTIVE:\n            wandb.log({\n                \"epoch\": epoch, \"train_loss\": train_loss, \"train_accuracy\": train_acc,\n                \"test_loss\": test_loss, \"test_accuracy\": test_acc,\n                \"learning_rate\": scheduler.get_last_lr()[0]\n            })\n\n        # Save a checkpoint to W&B Artifacts periodically (e.g., every 25 epochs)\n        if WANDB_ACTIVE and ((epoch + 1) % 25 == 0 or epoch == EPOCHS - 1):\n            # First, save the file locally in the Kaggle environment\n            local_path = f\"/kaggle/working/checkpoint_k{k}_epoch{epoch}.pth\"\n            torch.save({\n                'epoch': epoch, 'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(),\n            }, local_path)\n\n            # Create a W&B Artifact to version the model\n            artifact = wandb.Artifact(\n                name=f'densenet-cifar-k{k}', # Name for the artifact collection\n                type='model',\n                description=f'Checkpoint for Densenet k={k} at epoch {epoch}'\n            )\n            artifact.add_file(local_path)\n\n            # Upload the artifact to W&B servers\n            run.log_artifact(artifact)\n            print(f\"Checkpoint for epoch {epoch} saved to W&B Artifacts.\")\n\n    # Finish the W&B run\n    if WANDB_ACTIVE:\n        run.finish()\n\n    print(f\"--- Training finished for k = {k} ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T02:07:37.664074Z","iopub.execute_input":"2025-08-27T02:07:37.664337Z","iopub.status.idle":"2025-08-27T02:19:59.950999Z","shell.execute_reply.started":"2025-08-27T02:07:37.664316Z","shell.execute_reply":"2025-08-27T02:19:59.949751Z"}},"outputs":[{"name":"stdout","text":"##################################################\n--- Starting training for k = 12 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250827_020739-3n47xzr9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/img_seg/densenet-cifar10/runs/3n47xzr9' target=\"_blank\">densenet_k12</a></strong> to <a href='https://wandb.ai/img_seg/densenet-cifar10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/img_seg/densenet-cifar10' target=\"_blank\">https://wandb.ai/img_seg/densenet-cifar10</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/img_seg/densenet-cifar10/runs/3n47xzr9' target=\"_blank\">https://wandb.ai/img_seg/densenet-cifar10/runs/3n47xzr9</a>"},"metadata":{}},{"name":"stdout","text":"\nEpoch: 0 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"Loss: 1.456 | Acc: 46.042%: 100%|██████████| 391/391 [01:08<00:00,  5.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"--- Epoch 0 Test Results ---\nAccuracy: 52.48% | Error Rate: 47.52% | Loss: 1.51%\n--------------------------\n\nEpoch: 1 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [01:06<00:00,  5.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 2 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [01:06<00:00,  5.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 3 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [01:06<00:00,  5.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 4 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [01:06<00:00,  5.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 5 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [01:06<00:00,  5.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 6 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [01:06<00:00,  5.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 7 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [01:06<00:00,  5.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 8 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [01:06<00:00,  5.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 9 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [01:07<00:00,  5.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch: 10 | LR: 0.10000\n","output_type":"stream"},{"name":"stderr","text":"Loss: 0.339 | Acc: 88.275%:  33%|███▎      | 129/391 [00:22<00:45,  5.74it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2978604395.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Training and evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_cifar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/985533232.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, trainloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14},{"cell_type":"markdown","source":"## 2. For ImageNet","metadata":{"id":"_WDhqeB5rr2o"}},{"cell_type":"code","source":"# DATA LOADING AND TRANSFORMATION\n\n# ImageNet statistics\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Data augmentation for the training set\ntransform_train = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    normalize,\n])\n\n# Transformation for the validation set\ntransform_val = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    normalize,\n])","metadata":{"id":"rGBy4DvjrPOP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TRAINING FUNCTION\ndef train_imagenet(epoch, model, trainloader, optimizer, criterion, device):\n    print(f'\\nEpoch: {epoch}')\n    model.train()\n    train_loss = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        total += targets.size(0)\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch {epoch} | Batch {batch_idx}/{len(trainloader)} | Loss: {train_loss/(batch_idx+1):.3f}')\n\n# EVALUATION FUNCTION (WITH TOP-1 AND TOP-5)\ndef evaluate_imagenet(model, valloader, criterion, device):\n    model.eval()\n    val_loss = 0\n    correct_top1 = 0\n    correct_top5 = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(valloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            val_loss += loss.item()\n\n            # Calculate Top-1 and Top-5 accuracy\n            _, pred = outputs.topk(5, 1, largest=True, sorted=True)\n            pred = pred.t()\n            correct = pred.eq(targets.view(1, -1).expand_as(pred))\n\n            correct_top1 += correct[:1].reshape(-1).float().sum(0, keepdim=True).item()\n            correct_top5 += correct[:5].reshape(-1).float().sum(0, keepdim=True).item()\n            total += targets.size(0)\n\n    # Calculate final accuracies\n    top1_acc = 100. * correct_top1 / total\n    top5_acc = 100. * correct_top5 / total\n\n    print(\"\\n--- Validation Results ---\")\n    print(f\"Average Loss: {val_loss / len(valloader):.4f}\")\n    print(f\"Top-1 Accuracy: {top1_acc:.2f}% ({int(correct_top1)}/{total})\")\n    print(f\"Top-5 Accuracy: {top5_acc:.2f}% ({int(correct_top5)}/{total})\")\n    print(\"--------------------------\\n\")","metadata":{"id":"3KwZPBLXrWQ6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"INPUT_DIR = \"/kaggle/input/imagenet-object-localization-challenge\"\nOUTPUT_DIR = \"/kaggle/working/\"\n\n# This function will create and return the path to the sorted validation set\nsorted_val_path = prepare_validation_set_from_kaggle(input_dir=INPUT_DIR, output_dir=OUTPUT_DIR)\n\n# The training directory points DIRECTLY to the read-only input data. No copy needed!\ntrain_dir = os.path.join(INPUT_DIR, 'ILSVRC/Data/CLS-LOC/train')\n# The validation directory points to our newly created sorted folder\nval_dir = sorted_val_path\n\nprint(f\"\\nUsing Training data from: {train_dir}\")\nprint(f\"Using Validation data from: {val_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DataLoaders\ntrain_dataset = torchvision.datasets.ImageFolder(root=train_dir, transform=transform_train)\ntrainloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n\nval_dataset = torchvision.datasets.ImageFolder(root=val_dir, transform=transform_val)\nvalloader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=False, num_workers=4)\n\n# Use our from-scratch model, adjusting for the number of classes in the dataset\nnum_classes = len(train_dataset.classes)\nmodel = DenseNet121(num_classes=num_classes).to(device)\nprint(f\"Custom DenseNet-121 model for ImageNet created successfully with {num_classes} classes.\")\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n\n# Training Loop\nbest_acc = 0.0\nfor epoch in range(90):\n  train(epoch, model, trainloader, optimizer, criterion, device)\n  evaluate_imagenet(model, valloader, criterion, device)\n  scheduler.step()\n  # if acc > best_acc:\n  #     print(\"Saving new best model...\")\n  #     best_acc = acc\n  #     torch.save(model.state_dict(), 'densenet_imagenet_scratch_best.pth')","metadata":{"id":"xwQZnlI1sEtB","trusted":true},"outputs":[],"execution_count":null}]}